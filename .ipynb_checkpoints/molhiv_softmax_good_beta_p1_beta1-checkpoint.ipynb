{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "OBxXkQOYvhNz",
    "outputId": "43f1acc3-0082-4f3e-d923-7788da74b19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (4.48.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.6.0+cu101)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.21.3)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.19.1)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.25.10)\n",
      "Requirement already satisfied: future in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from torch>=1.2.0->ogb) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.24.0)\n",
      "Requirement already satisfied: littleutils in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2020.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6oAevLez2il"
   },
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_softmax, scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13A6KgTVui6Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_scatter import scatter, scatter_mean, scatter_add, scatter_sum\n",
    "from torch_geometric.nn import GINConv, GINEConv\n",
    "\n",
    "\n",
    "class AtomEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(9):\n",
    "            self.embeddings.append(Embedding(100, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(x.size(1)):\n",
    "            out += self.embeddings[i](x[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class BondEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(3):\n",
    "            self.embeddings.append(Embedding(6, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        if edge_attr.dim() == 1:\n",
    "            edge_attr = edge_attr.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(edge_attr.size(1)):\n",
    "            out += self.embeddings[i](edge_attr[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class Global_Gen_Sum_Mean_Max_Pool(nn.Module):\n",
    "    def __init__(self, family = \"softmax\", p = 1.0, beta = 1.0, \n",
    "                 trainable_p = False, trainable_beta = False):\n",
    "        r\"\"\"Performs batch-wise graph-level-outputs by transforming node\n",
    "        features based on a Generalized Aggr-Mean-Max function, so that\n",
    "        for a single graph :math:`\\mathcal{G}_i` its output is computed\n",
    "        deppending on the family of transformations by:\n",
    "        .. math::\n",
    "            \\mathbf{r}_i = \\frac{1}{\\beta*N_i} \\sum_{n=1}^{N_i} \\mathbf{softmax} \\left( \\mathbf{x}_n * p \\right) * \\mathbf{x}_n\n",
    "        for softmax aggregation or\n",
    "        .. math::\n",
    "            \\mathbf{r}_i = \\left( \\frac{1}{\\beta*N_i} \\sum_{n=1}^{N_i} \\mathbf{x}_n^{p} \\right)^{1/p}\n",
    "        for power mean aggregation.\n",
    "\n",
    "        Args:\n",
    "            family (str): family of generalized mean-max functions to use. \n",
    "                Either \"softmax\" or \"power\" for eq. 1 or eq. 2 respectively.\n",
    "            p (float): parameter for the generalized mean-max function\n",
    "            trainable (bool): whether the value of p is learnable during training.\n",
    "        \"\"\"\n",
    "        super(Global_Gen_Sum_Mean_Max_Pool, self).__init__()\n",
    "        \n",
    "        self.family         = family\n",
    "        self.base_p         = p\n",
    "        self.base_beta      = beta\n",
    "        self.trainable_p    = trainable_p\n",
    "        self.trainable_beta = trainable_beta\n",
    "        # define params\n",
    "        self.p = torch.nn.Parameter(torch.tensor([p], device=device),\n",
    "                                    requires_grad=trainable_p)# .to(device)\n",
    "        self.beta = torch.nn.Parameter(torch.tensor([beta], device=device),\n",
    "                                       requires_grad=trainable_beta)# .to(device)\n",
    "\n",
    "    def forward(self, x, batch, bsize=None):\n",
    "        r\"\"\"Args:\n",
    "            x (Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}`.\n",
    "            batch (LongTensor): Batch vector :math:`\\mathbf{b} \\in {\\{ 0, \\ldots,\n",
    "                B-1\\}}^N`, which assigns each node to a specific example.\n",
    "            size (int, optional): Batch-size :math:`B`.\n",
    "                Automatically calculated if not given. (default: :obj:`None`)\n",
    "        :rtype: :class:`Tensor`\n",
    "        \"\"\"\n",
    "        bsize = int(batch.max().item() + 1) if bsize is None else bsize\n",
    "        n_nodes = scatter_sum(torch.ones_like(x), batch, dim=0, dim_size=bsize)\n",
    "        if self.family == \"softmax\":\n",
    "            out = scatter_softmax(self.p * x.detach(), batch, dim=0)\n",
    "            return scatter_add(x * out,\n",
    "                                batch, dim=0, dim_size=bsize)*n_nodes / (1+self.beta*(n_nodes-1))\n",
    "\n",
    "        elif self.family == \"power\":\n",
    "            # numerical stability - avoid powers of large numbers or negative ones\n",
    "            min_x, max_x = 1e-7, 1e+3\n",
    "            torch.clamp_(x, min_x, max_x)\n",
    "            out = scatter_add(torch.pow(x, self.p),\n",
    "                               batch, dim=0, dim_size=bsize) / (1+self.beta*(n_nodes-1))\n",
    "            torch.clamp_(out, min_x, max_x)\n",
    "            return torch.pow(out, 1 / self.p)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.p and torch.is_tensor(self.p):\n",
    "            self.p.data.fill_(self.base_p)\n",
    "        if self.beta and torch.is_tensor(self.beta):\n",
    "            self.beta.data.fill_(self.base_beta)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Generalized Aggr-Mean-Max global pooling layer with params:\" + \\\n",
    "               str({\"family\": self.family,\n",
    "                    \"base_p\": self.base_p,\n",
    "                    \"base_beta\"     : self.base_beta,\n",
    "                    \"trainable_p\"   : self.trainable_p,\n",
    "                    \"trainable_beta\": self.trainable_beta})\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, dropout=0.0,\n",
    "                 inter_message_passing=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.inter_message_passing = inter_message_passing\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(hidden_channels)\n",
    "        self.clique_encoder = Embedding(4, hidden_channels)\n",
    "\n",
    "        self.bond_encoders = ModuleList()\n",
    "        self.atom_convs = ModuleList()\n",
    "        self.atom_batch_norms = ModuleList()\n",
    "        self.reader = Global_Gen_Sum_Mean_Max_Pool(family = \"softmax\", p = 1.0, beta = 1.0, \n",
    "                                                   trainable_p = True, trainable_beta = True)\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.bond_encoders.append(BondEncoder(hidden_channels))\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.atom_convs.append(GINEConv(nn, train_eps=True))\n",
    "            self.atom_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.clique_convs = ModuleList()\n",
    "        self.clique_batch_norms = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.clique_convs.append(GINConv(nn, train_eps=True))\n",
    "            self.clique_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.atom2clique_lins = ModuleList()\n",
    "        self.clique2atom_lins = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.atom2clique_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "            self.clique2atom_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.atom_lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.clique_lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.atom_encoder.reset_parameters()\n",
    "        self.clique_encoder.reset_parameters()\n",
    "\n",
    "        for emb, conv, batch_norm in zip(self.bond_encoders, self.atom_convs,\n",
    "                                         self.atom_batch_norms):\n",
    "            emb.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for conv, batch_norm in zip(self.clique_convs,\n",
    "                                    self.clique_batch_norms):\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for lin1, lin2 in zip(self.atom2clique_lins, self.clique2atom_lins):\n",
    "            lin1.reset_parameters()\n",
    "            lin2.reset_parameters()\n",
    "\n",
    "        self.atom_lin.reset_parameters()\n",
    "        self.clique_lin.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        self.reader.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.squeeze())\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            x_clique = self.clique_encoder(data.x_clique.squeeze())\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            edge_attr = self.bond_encoders[i](data.edge_attr)\n",
    "            x = self.atom_convs[i](x, data.edge_index, edge_attr)\n",
    "            x = self.atom_batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "            if self.inter_message_passing:\n",
    "                row, col = data.atom2clique_index\n",
    "\n",
    "                x_clique = x_clique + F.relu(self.atom2clique_lins[i](scatter(\n",
    "                    x[row], col, dim=0, dim_size=x_clique.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "                x_clique = self.clique_convs[i](x_clique, data.tree_edge_index)\n",
    "                x_clique = self.clique_batch_norms[i](x_clique)\n",
    "                x_clique = F.relu(x_clique)\n",
    "                x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                     training=self.training)\n",
    "\n",
    "                x = x + F.relu(self.clique2atom_lins[i](scatter(\n",
    "                    x_clique[col], row, dim=0, dim_size=x.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "        x = self.reader(x, data.batch)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.atom_lin(x)\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            tree_batch = torch.repeat_interleave(data.num_cliques)\n",
    "            x_clique = scatter(x_clique, tree_batch, dim=0, dim_size=x.size(0),\n",
    "                               reduce='mean')\n",
    "            x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                 training=self.training)\n",
    "            x_clique = self.clique_lin(x_clique)\n",
    "            x = x + x_clique\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkP00CdeukM5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import tree_decomposition\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import BondType\n",
    "\n",
    "bonds = [BondType.SINGLE, BondType.DOUBLE, BondType.TRIPLE, BondType.AROMATIC]\n",
    "\n",
    "\n",
    "def mol_from_data(data):\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    x = data.x if data.x.dim() == 1 else data.x[:, 0]\n",
    "    for z in x.tolist():\n",
    "        mol.AddAtom(Chem.Atom(z))\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask].tolist(), col[mask].tolist()\n",
    "\n",
    "    bond_type = data.edge_attr\n",
    "    bond_type = bond_type if bond_type.dim() == 1 else bond_type[:, 0]\n",
    "    bond_type = bond_type[mask].tolist()\n",
    "\n",
    "    for i, j, bond in zip(row, col, bond_type):\n",
    "        assert bond >= 1 and bond <= 4\n",
    "        mol.AddBond(i, j, bonds[bond - 1])\n",
    "\n",
    "    return mol.GetMol()\n",
    "\n",
    "\n",
    "class JunctionTreeData(Data):\n",
    "    def __inc__(self, key, item):\n",
    "        if key == 'tree_edge_index':\n",
    "            return self.x_clique.size(0)\n",
    "        elif key == 'atom2clique_index':\n",
    "            return torch.tensor([[self.x.size(0)], [self.x_clique.size(0)]])\n",
    "        else:\n",
    "            return super(JunctionTreeData, self).__inc__(key, item)\n",
    "\n",
    "\n",
    "class JunctionTree(object):\n",
    "    def __call__(self, data):\n",
    "        mol = mol_from_data(data)\n",
    "        out = tree_decomposition(mol, return_vocab=True)\n",
    "        tree_edge_index, atom2clique_index, num_cliques, x_clique = out\n",
    "\n",
    "        data = JunctionTreeData(**{k: v for k, v in data})\n",
    "\n",
    "        data.tree_edge_index = tree_edge_index\n",
    "        data.atom2clique_index = atom2clique_index\n",
    "        data.num_cliques = num_cliques\n",
    "        data.x_clique = x_clique\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-BnRJKkukQ8"
   },
   "outputs": [],
   "source": [
    "# edit the function causing the error: add argument chem=None + modify function code: \n",
    "# Chem=chem if chem is not None else Chem\n",
    "\n",
    "# tree_decomposition(Chem.MolFromSmiles(\"cicccc1c\"), return_vocab=True)\n",
    "# once modified and saved, restart the environmnet, comment this cell and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "l-eGz7HtCLCr",
    "outputId": "a5812a61-fea7-4301-dd16-9d75ea62035b"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "class Argparse_emulate():\n",
    "    def __init__(self, device=0, hidden_channels=256, num_layers=2, dropout=0.5,\n",
    "               epochs=100, no_inter_message_passing=\"store_true\"):\n",
    "        self.device = device\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.no_inter_message_passing = no_inter_message_passing\n",
    "        return\n",
    "\n",
    "args = Argparse_emulate()\n",
    "# parser.add_argument('--device', type=int, default=0)\n",
    "# parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "# parser.add_argument('--num_layers', type=int, default=2)\n",
    "# parser.add_argument('--dropout', type=float, default=0.5)\n",
    "# parser.add_argument('--epochs', type=int, default=100)\n",
    "# parser.add_argument('--no_inter_message_passing', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "\n",
    "class OGBTransform(object):\n",
    "    # OGB saves atom and bond types zero-index based. We need to revert that.\n",
    "    def __call__(self, data):\n",
    "        data.x[:, 0] += 1\n",
    "        data.edge_attr[:, 0] += 1\n",
    "        return data\n",
    "\n",
    "\n",
    "transform = Compose([OGBTransform(), JunctionTree()])\n",
    "\n",
    "name = 'ogbg-molhiv'\n",
    "dataset = PygGraphPropPredDataset(name, 'data', pre_transform=transform)\n",
    "\n",
    "# correct splits\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]\n",
    "val_dataset = dataset[split_idx['valid']]\n",
    "test_dataset = dataset[split_idx['test']]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, 128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False)\n",
    "\n",
    "\n",
    "device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(epoch, vals=False):\n",
    "    values = []\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = ~torch.isnan(data.y)\n",
    "        out = model(data)[mask]\n",
    "        y = data.y.to(torch.float)[mask]\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(out, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        # record beta and p values\n",
    "        if vals:\n",
    "            # display computational graph\n",
    "            # global g\n",
    "            #g = make_dot(out)\n",
    "            # \"a\"+9\n",
    "            values_batch = {}\n",
    "            values_batch[\"p\"]    = model.reader.p.detach().cpu().numpy()\n",
    "            values_batch[\"beta\"] = model.reader.beta.detach().cpu().numpy()\n",
    "            values.append(values_batch)\n",
    "            if False: # i==0:\n",
    "                print(\"records:\", i, \"value:\", values_batch)\n",
    "\n",
    "\n",
    "    return total_loss / len(train_loader.dataset), values\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_preds, y_trues = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        y_preds.append(model(data))\n",
    "        y_trues.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(y_preds, dim=0).cpu().numpy()\n",
    "    y_true = torch.cat(y_trues, dim=0).cpu().numpy()\n",
    "\n",
    "    rocauc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        # AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:\n",
    "            # ignore nan values\n",
    "            is_labeled = y_true[:, i] == y_true[:, i]\n",
    "            rocauc_list.append(\n",
    "                roc_auc_score(y_true[is_labeled, i], y_pred[is_labeled, i]))\n",
    "\n",
    "    return {\"rocauc\": sum(rocauc_list) / len(rocauc_list)}\n",
    "\n",
    "\n",
    "values     = []\n",
    "test_perfs = []\n",
    "for run in range(10):\n",
    "    print()\n",
    "    print(f'Run {run}:')\n",
    "    print()\n",
    "    model = Net(hidden_channels=args.hidden_channels,\n",
    "            out_channels=dataset.num_tasks, num_layers=args.num_layers,\n",
    "            dropout = args.dropout if run<10 else 0.6, # edited to increase dropout\n",
    "            inter_message_passing=not args.no_inter_message_passing).to(device)\n",
    "\n",
    "    model.reset_parameters()\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    best_val_perf = test_perf = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss, epoch_values = train(epoch, vals=True)\n",
    "        train_perf = test(train_loader)\n",
    "        val_perf = test(val_loader)\n",
    "\n",
    "        if val_perf[\"rocauc\"] > best_val_perf:\n",
    "            best_val_perf = val_perf[\"rocauc\"]\n",
    "            test_perf = test(test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_perf[\"rocauc\"]:.4f}, Val: {val_perf[\"rocauc\"]:.4f}, '\n",
    "              f'Test: {test_perf[\"rocauc\"]:.4f}')\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Recorded values:\", epoch_values)\n",
    "\n",
    "    test_perfs.append(test_perf[\"rocauc\"])\n",
    "    values.append(epoch_values[-1])\n",
    "\n",
    "test_perf = torch.tensor(test_perfs)\n",
    "print('===========================')\n",
    "print(f'Final Test: {test_perf.mean():.4f} ± {test_perf.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p_epoch    = np.array([batch[\"p\"] for batch in epoch_values]).flatten()\n",
    "beta_epoch = np.array([batch[\"beta\"] for batch in epoch_values]).flatten() \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"Evolution of parameters beta and p during training - Power Mean\")\n",
    "plt.plot(p_epoch, \"r-\", label=\"p\")\n",
    "plt.plot(beta_epoch, \"b-\", label=\"beta\")\n",
    "plt.ylim(0, 1.1*np.amax(np.maximum(p_epoch, beta_epoch)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de analysis_molhiv_power.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
